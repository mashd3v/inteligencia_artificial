{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "b59879d5e3f5a8a2f2faee07a70ee8a2e6ae4b34825721b6f60a87942e756af3"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Etiquetado en NLTK"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Pipeline basico para el ingles"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\MICHU\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\MICHU\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#@title Dependencias previas\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('here', 'RB'),\n",
       " ('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('enjoying', 'VBG'),\n",
       " ('today', 'NN')]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "#@title Etiquetado en una linea\n",
    "text = word_tokenize('And now here I am enjoying today')\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "None\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "None\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "None\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\MICHU\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#@title Categoria gramatical de cada etiqueta\n",
    "nltk.download('tagsets')\n",
    "for tag in ['CC', 'RB', 'PRP']:\n",
    "    print(nltk.help.upenn_tagset(tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('They', 'PRP'),\n",
       " ('do', 'VBP'),\n",
       " ('not', 'RB'),\n",
       " ('permit', 'VB'),\n",
       " ('other', 'JJ'),\n",
       " ('people', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('get', 'VB'),\n",
       " ('residence', 'NN'),\n",
       " ('permit', 'NN')]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "#@title Palabras homonimas\n",
    "text = word_tokenize('They do not permit other people to get residence permit')\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "source": [
    "# Etiquetado en espanol\n",
    "\n",
    "Para el ingles, NLTK tiene tokenizador y etiquetador pre-entrenados por defecto. En cambio, para otros idiomas es preciso entrenarlo previamente.\n",
    "- usamos el corpus cess_esp\n",
    "- el cual usa una convencion de etiquetas gramaticales dada por el grupo EAGLES"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package cess_esp to\n[nltk_data]     C:\\Users\\MICHU\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package cess_esp is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('cess_esp')\n",
    "from nltk.corpus import cess_esp as cess\n",
    "from nltk import UnigramTagger as ut \n",
    "from nltk import BigramTagger as bt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8068832283915284"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "#@title Entrenamiento del tagger por unigramas\n",
    "cess_sents = cess.tagged_sents()\n",
    "fraction = int(len(cess_sents) * 90 / 100)\n",
    "uni_tagger = ut(cess_sents[:fraction])\n",
    "uni_tagger.evaluate(cess_sents[fraction:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Yo', 'pp1csn00'),\n",
       " ('soy', 'vsip1s0'),\n",
       " ('una', 'di0fs0'),\n",
       " ('persona', 'ncfs000'),\n",
       " ('muy', 'rg'),\n",
       " ('amable', None)]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "uni_tagger.tag('Yo soy una persona muy amable'.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.1095272206303725"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "#@title Entrenamiento del tagger por bigramas\n",
    "fraction = int(len(cess_sents) * 90 / 100)\n",
    "bi_tagger = bt(cess_sents[:fraction])\n",
    "bi_tagger.evaluate(cess_sents[fraction + 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Yo', 'pp1csn00'),\n",
       " ('soy', 'vsip1s0'),\n",
       " ('una', None),\n",
       " ('persona', None),\n",
       " ('muy', None),\n",
       " ('amable', None)]"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# no se recomienda usar\n",
    "bi_tagger.tag('Yo soy una persona muy amable'.split(' '))"
   ]
  },
  {
   "source": [
    "# Etiquetado mejorado con Stanza (Stanford)\n",
    "\n",
    "### Que es estanza?\n",
    "- El grupo de investigacion en NLP de Stanford tenia una suite de librerias que ejecutaban varias tareas de NLP, esta suite se unifico en un solo servicio que le llamaron **CoreNLP** con base en codigo java: https://stanfordnlp.github.io/CoreNLP/index.html\n",
    "- Para python existe StanfordNLP: https://stanfordnlp.github.io/stanfordnlp/index.html \n",
    "- Sin embargo, StanfodNLP ha sido deprecada y las nuevas versiones de la suite de NLP reciben mantenimiento bajo el nombre de Stanza: https://stanfordnlp.github.io/stanza"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting stanza\n",
      "  Downloading stanza-1.2-py3-none-any.whl (282 kB)\n",
      "Requirement already satisfied: protobuf in c:\\users\\michu\\appdata\\roaming\\python\\python37\\site-packages (from stanza) (3.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\michu\\appdata\\roaming\\python\\python37\\site-packages (from stanza) (1.17.1)\n",
      "Requirement already satisfied: tqdm in a:\\anaconda3\\lib\\site-packages (from stanza) (4.32.1)\n",
      "Collecting torch>=1.3.0\n",
      "  Downloading torch-1.8.0-cp37-cp37m-win_amd64.whl (190.5 MB)\n",
      "Requirement already satisfied: requests in a:\\anaconda3\\lib\\site-packages (from stanza) (2.22.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\michu\\appdata\\roaming\\python\\python37\\site-packages (from protobuf->stanza) (41.2.0)\n",
      "Requirement already satisfied: six>=1.9 in a:\\anaconda3\\lib\\site-packages (from protobuf->stanza) (1.12.0)\n",
      "Collecting typing-extensions\n",
      "  Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in a:\\anaconda3\\lib\\site-packages (from requests->stanza) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in a:\\anaconda3\\lib\\site-packages (from requests->stanza) (2019.6.16)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in a:\\anaconda3\\lib\\site-packages (from requests->stanza) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in a:\\anaconda3\\lib\\site-packages (from requests->stanza) (1.24.2)\n",
      "Installing collected packages: typing-extensions, torch, stanza\n",
      "Successfully installed stanza-1.2 torch-1.8.0 typing-extensions-3.7.4.3\n",
      "WARNING: You are using pip version 20.0.2; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the 'a:\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 16.0MB/s]                    \n",
      "2021-03-10 12:33:37 INFO: Downloading default packages for language: es (Spanish)...\n",
      "Downloading http://nlp.stanford.edu/software/stanza/1.2.0/es/default.zip: 100%|██████████| 566M/566M [01:48<00:00, 5.24MB/s]\n",
      "2021-03-10 12:35:34 INFO: Finished downloading models and saved to C:\\Users\\MICHU\\stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-03-10 13:06:40 WARNING: Language es package default expects mwt, which has been added\n",
      "2021-03-10 13:06:40 INFO: Loading these models for language: es (Spanish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ancora  |\n",
      "| mwt       | ancora  |\n",
      "| pos       | ancora  |\n",
      "=======================\n",
      "\n",
      "2021-03-10 13:06:40 INFO: Use device: cpu\n",
      "2021-03-10 13:06:40 INFO: Loading: tokenize\n",
      "2021-03-10 13:06:40 INFO: Loading: mwt\n",
      "2021-03-10 13:06:40 INFO: Loading: pos\n",
      "2021-03-10 13:06:41 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline('es', processors='tokenize, pos')\n",
    "doc = nlp('Yo soy una persona muy amable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Yo PRON\n",
      "soy AUX\n",
      "una DET\n",
      "persona NOUN\n",
      "muy ADV\n",
      "amable ADJ\n"
     ]
    }
   ],
   "source": [
    "for snetence in doc.sentences:\n",
    "    for word in snetence.words:\n",
    "        print(word.text, word.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}