{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "b59879d5e3f5a8a2f2faee07a70ee8a2e6ae4b34825721b6f60a87942e756af3"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Ahora que ya sabemos para el algoritmo de PCA, ¿que otras alternativas tenemos?\n",
    "\n",
    "Bueno, una alternativa son los Kernels. Un Kernel es una función matemática que toma mediciones que se comportan de manera no lineal y las proyecta en un espacio dimensional más grande en donde sen linealmente separables.\n",
    "\n",
    "### Y, ¿esto para que puede servir?\n",
    "\n",
    "![](./images/kernels.png)\n",
    "\n",
    "Sirve para casos en donde no son linealmente separables. El la primera imagen no es posible separarlos con una linea y en la imagen 2 si lo podemos hacer mediante Kernels. Lo que hace la función de Kernels es proyectar los puntos en otra dimensión y así volver los datos linealmente separables.\n",
    "\n",
    "### ¿Que tipo de funciones para Kernels nos podemos encontrar?\n",
    "\n",
    "![](./images/funciones.png)\n",
    "\n",
    "### Ejemplos de funciones de kernels en datasets aplicados a un clasificador:\n",
    "\n",
    "![](./images/kernels_aplicados.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   52    1   0       125   212    0        1      168      0      1.0      2   \n",
       "1   53    1   0       140   203    1        0      155      1      3.1      0   \n",
       "2   70    1   0       145   174    0        1      125      1      2.6      0   \n",
       "3   61    1   0       148   203    0        1      161      0      0.0      2   \n",
       "4   62    0   0       138   294    1        1      106      0      1.9      1   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   2     3       0  \n",
       "1   0     3       0  \n",
       "2   0     3       0  \n",
       "3   1     3       0  \n",
       "4   3     2       0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>sex</th>\n      <th>cp</th>\n      <th>trestbps</th>\n      <th>chol</th>\n      <th>fbs</th>\n      <th>restecg</th>\n      <th>thalach</th>\n      <th>exang</th>\n      <th>oldpeak</th>\n      <th>slope</th>\n      <th>ca</th>\n      <th>thal</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>52</td>\n      <td>1</td>\n      <td>0</td>\n      <td>125</td>\n      <td>212</td>\n      <td>0</td>\n      <td>1</td>\n      <td>168</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>53</td>\n      <td>1</td>\n      <td>0</td>\n      <td>140</td>\n      <td>203</td>\n      <td>1</td>\n      <td>0</td>\n      <td>155</td>\n      <td>1</td>\n      <td>3.1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>70</td>\n      <td>1</td>\n      <td>0</td>\n      <td>145</td>\n      <td>174</td>\n      <td>0</td>\n      <td>1</td>\n      <td>125</td>\n      <td>1</td>\n      <td>2.6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>61</td>\n      <td>1</td>\n      <td>0</td>\n      <td>148</td>\n      <td>203</td>\n      <td>0</td>\n      <td>1</td>\n      <td>161</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>62</td>\n      <td>0</td>\n      <td>0</td>\n      <td>138</td>\n      <td>294</td>\n      <td>1</td>\n      <td>1</td>\n      <td>106</td>\n      <td>0</td>\n      <td>1.9</td>\n      <td>1</td>\n      <td>3</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Importando nuestro dataset\n",
    "dt_heart = pd.read_csv('./documents/heart.csv')\n",
    "dt_heart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir el dataset en features y target\n",
    "dt_features = dt_heart.drop(['target'], axis=1)\n",
    "dt_target = dt_heart['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar datos con StandardScaler\n",
    "dt_features = StandardScaler().fit_transform(dt_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividr nuestro dataset en datos de entrenamiento y datos de test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(dt_features, dt_target, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "KernelPCA(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto',\n",
       "          fit_inverse_transform=False, gamma=None, kernel='poly',\n",
       "          kernel_params=None, max_iter=None, n_components=4, n_jobs=None,\n",
       "          random_state=None, remove_zero_eig=False, tol=0)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# Creamos nuestro KPCA\n",
    "kpca = KernelPCA(n_components=4, kernel='poly')\n",
    "kpca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicarndo el algoritmo de KPCA a nuestro datos\n",
    "dt_train = kpca.transform(X_train)\n",
    "dt_test = kpca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar regresion logistica\n",
    "logistic = LogisticRegression(solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# Mandamos los data frames la la regresión logística\n",
    "logistic.fit(dt_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SCORE PCA:  0.8409090909090909\n"
     ]
    }
   ],
   "source": [
    "#Calculamos nuestra exactitud de nuestra predicción\n",
    "print(\"SCORE PCA: \", logistic.score(dt_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}